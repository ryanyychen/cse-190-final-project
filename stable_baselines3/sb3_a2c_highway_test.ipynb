{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972d8446",
   "metadata": {},
   "source": [
    "# CHANGE THESE CONFIGS, THEN UPDATE MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8810a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models/highway/a2c/a2c\"\n",
    "TENSORBOARD_LOG_DIR = \"./models/highway/a2c/logs\"\n",
    "IMAGE_TAG = \"a2c_rewards_highway\"\n",
    "IMAGE_DIR = \"./images/highway/a2c\"\n",
    "RUNS_FILE = \"./models/highway/a2c/a2c_success_runs.pkl\"\n",
    "\n",
    "#### ENV CONFIGS ####\n",
    "CONFIG = {\n",
    "    \"observation\": {\"type\": \"Kinematics\"},\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\",\n",
    "                },\n",
    "                \"lanes_count\": 4,\n",
    "                \"vehicles_count\": 50,\n",
    "                \"controlled_vehicles\": 1,\n",
    "                \"initial_lane_id\": None,\n",
    "                \"duration\": 40,  # [s]\n",
    "                \"ego_spacing\": 2,\n",
    "                \"vehicles_density\": 1,\n",
    "                \"collision_reward\": -1,  # The reward received when colliding with a vehicle.\n",
    "                \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "                # zero for other lanes.\n",
    "                \"high_speed_reward\": 0.4,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "                # lower speeds according to config[\"reward_speed_range\"].\n",
    "                \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "                \"reward_speed_range\": [20, 30],\n",
    "                \"normalize_reward\": True,\n",
    "                \"offroad_terminal\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698ca84",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5b59c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
    "\n",
    "from custom_highway_env import CustomHighwayEnv\n",
    "from custom_training_callback import RewardTrackingCallback\n",
    "# import highway_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33468fde",
   "metadata": {},
   "source": [
    "## Register Env with Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a76e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id='custom-highway-v0',\n",
    "    entry_point='custom_highway_env:CustomHighwayEnv',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea952aa",
   "metadata": {},
   "source": [
    "## Create and Wrap Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"custom-highway-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# env = gym.make(\"highway-v0\", render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d79e0f",
   "metadata": {},
   "source": [
    "## UPDATE HERE: Set Up Correct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a151beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    n_steps=5,\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    rms_prop_eps=1e-5,\n",
    "    use_rms_prop=True,\n",
    "    normalize_advantage=False,\n",
    "    tensorboard_log=TENSORBOARD_LOG_DIR,\n",
    "    verbose=0,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ccb7b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f8276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319c3d06ec9947fd98a3810b15be2061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_callback = RewardTrackingCallback(\n",
    "    tag=IMAGE_TAG,\n",
    "    path_dir=IMAGE_DIR\n",
    ")\n",
    "\n",
    "steps = 5000\n",
    "config = CONFIG.copy()\n",
    "env = gym.make(\"custom-highway-v0\", render_mode='rgb_array', config=config)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model.set_env(env)  # Update the model with the new environment\n",
    "model.learn(\n",
    "    total_timesteps=steps,\n",
    "    callback=[ProgressBarCallback(), reward_callback]\n",
    ")\n",
    "reward_callback.start_new_phase()\n",
    "reward_callback.save_all_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4906",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f66b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578176a8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c1302",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2030a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb9ebe",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eeefa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished, total reward: 26.694148001393476\n",
      "Episode 2 finished, total reward: 29.360814668060133\n",
      "Episode 3 finished, total reward: 27.58303689028236\n",
      "Episode 4 finished, total reward: 26.694148001393476\n",
      "Episode 5 finished, total reward: 26.694148001393476\n",
      "Episode 6 finished, total reward: 29.360814668060133\n",
      "Episode 7 finished, total reward: 28.471925779171247\n",
      "Episode 8 finished, total reward: 27.58303689028236\n",
      "Episode 9 finished, total reward: 27.58303689028236\n",
      "Episode 10 finished, total reward: 26.694148001393476\n",
      "Episode 11 finished, total reward: 28.471925779171247\n",
      "Episode 12 finished, total reward: 26.694148001393476\n",
      "Episode 13 finished, total reward: 28.471925779171247\n",
      "Episode 14 finished, total reward: 29.360814668060133\n",
      "Episode 15 finished, total reward: 27.58303689028236\n",
      "Episode 16 finished, total reward: 28.471925779171247\n",
      "Episode 17 finished, total reward: 27.58303689028236\n",
      "Episode 18 finished, total reward: 29.360814668060133\n",
      "Episode 19 finished, total reward: 26.694148001393476\n",
      "Episode 20 finished, total reward: 27.58303689028236\n",
      "Episode 21 finished, total reward: 26.694148001393476\n",
      "Episode 22 finished, total reward: 27.58303689028236\n",
      "Episode 23 finished, total reward: 26.694148001393476\n",
      "Episode 24 finished, total reward: 28.471925779171247\n",
      "Episode 25 finished, total reward: 27.58303689028236\n",
      "Episode 26 finished, total reward: 27.58303689028236\n",
      "Episode 27 finished, total reward: 26.694148001393476\n",
      "Episode 28 finished, total reward: 28.471925779171247\n",
      "Episode 29 finished, total reward: 29.360814668060133\n",
      "Episode 30 finished, total reward: 27.58303689028236\n",
      "Episode 31 finished, total reward: 26.694148001393476\n",
      "Episode 32 finished, total reward: 28.471925779171247\n",
      "Episode 33 finished, total reward: 29.360814668060133\n",
      "Episode 34 finished, total reward: 29.360814668060133\n",
      "Episode 35 finished, total reward: 29.360814668060133\n",
      "Episode 36 finished, total reward: 26.694148001393476\n",
      "Episode 37 finished, total reward: 29.360814668060133\n",
      "Episode 38 finished, total reward: 27.58303689028236\n",
      "Episode 39 finished, total reward: 27.58303689028236\n",
      "Episode 40 finished, total reward: 26.694148001393476\n",
      "Episode 41 finished, total reward: 29.360814668060133\n",
      "Episode 42 finished, total reward: 28.471925779171247\n",
      "Episode 43 finished, total reward: 26.694148001393476\n",
      "Episode 44 finished, total reward: 26.694148001393476\n",
      "Episode 45 finished, total reward: 26.694148001393476\n",
      "Episode 46 finished, total reward: 29.360814668060133\n",
      "Episode 47 finished, total reward: 27.58303689028236\n",
      "Episode 48 finished, total reward: 29.360814668060133\n",
      "Episode 49 finished, total reward: 28.471925779171247\n",
      "Episode 50 finished, total reward: 29.360814668060133\n",
      "Episode 51 finished, total reward: 29.360814668060133\n",
      "Episode 52 finished, total reward: 28.471925779171247\n",
      "Episode 53 finished, total reward: 29.360814668060133\n",
      "Episode 54 finished, total reward: 29.360814668060133\n",
      "Episode 55 finished, total reward: 29.360814668060133\n",
      "Episode 56 finished, total reward: 28.471925779171247\n",
      "Episode 57 finished, total reward: 29.360814668060133\n",
      "Episode 58 finished, total reward: 29.360814668060133\n",
      "Episode 59 finished, total reward: 29.360814668060133\n",
      "Episode 60 finished, total reward: 28.471925779171247\n",
      "Episode 61 finished, total reward: 28.471925779171247\n",
      "Episode 62 finished, total reward: 27.58303689028236\n",
      "Episode 63 finished, total reward: 29.360814668060133\n",
      "Episode 64 finished, total reward: 28.471925779171247\n",
      "Episode 65 finished, total reward: 28.471925779171247\n",
      "Episode 66 finished, total reward: 29.360814668060133\n",
      "Episode 67 finished, total reward: 26.694148001393476\n",
      "Episode 68 finished, total reward: 27.58303689028236\n",
      "Episode 69 finished, total reward: 29.360814668060133\n",
      "Episode 70 finished, total reward: 27.58303689028236\n",
      "Episode 71 finished, total reward: 27.58303689028236\n",
      "Episode 72 finished, total reward: 27.58303689028236\n",
      "Episode 73 finished, total reward: 27.58303689028236\n",
      "Episode 74 finished, total reward: 26.694148001393476\n",
      "Episode 75 finished, total reward: 27.58303689028236\n",
      "Episode 76 finished, total reward: 27.58303689028236\n",
      "Episode 77 finished, total reward: 29.360814668060133\n",
      "Episode 78 finished, total reward: 29.360814668060133\n",
      "Episode 79 finished, total reward: 28.471925779171247\n",
      "Episode 80 finished, total reward: 29.360814668060133\n",
      "Episode 81 finished, total reward: 29.360814668060133\n",
      "Episode 82 finished, total reward: 27.58303689028236\n",
      "Episode 83 finished, total reward: 26.694148001393476\n",
      "Episode 84 finished, total reward: 29.360814668060133\n",
      "Episode 85 finished, total reward: 28.471925779171247\n",
      "Episode 86 finished, total reward: 29.360814668060133\n",
      "Episode 87 finished, total reward: 27.58303689028236\n",
      "Episode 88 finished, total reward: 27.58303689028236\n",
      "Episode 89 finished, total reward: 28.471925779171247\n",
      "Episode 90 finished, total reward: 29.360814668060133\n",
      "Episode 91 finished, total reward: 27.58303689028236\n",
      "Episode 92 finished, total reward: 28.471925779171247\n",
      "Episode 93 finished, total reward: 28.471925779171247\n",
      "Episode 94 finished, total reward: 27.58303689028236\n",
      "Episode 95 finished, total reward: 27.58303689028236\n",
      "Episode 96 finished, total reward: 29.360814668060133\n",
      "Episode 97 finished, total reward: 27.58303689028236\n",
      "Episode 98 finished, total reward: 26.694148001393476\n",
      "Episode 99 finished, total reward: 29.360814668060133\n",
      "Episode 100 finished, total reward: 29.360814668060133\n",
      "Total collisions: 0 out of 100 episodes\n",
      "Total destination arrivals: 0 out of 100 episodes\n",
      "No successful episodes.\n"
     ]
    }
   ],
   "source": [
    "collisions = 0\n",
    "destination_arrivals = 0\n",
    "success_count = 0\n",
    "successful_flopcount = 0\n",
    "episodes = 100\n",
    "\n",
    "# Store successful runs for rendering\n",
    "successful_runs = []\n",
    "\n",
    "for eps in range(100):\n",
    "    # config = CONFIG.copy()\n",
    "    # config[\"destination\"] = \"o\" + str(random.randint(1, 3))\n",
    "    # env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=config)\n",
    "    env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "\n",
    "    seed = random.randint(0, 10000)\n",
    "\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    episode_flops = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    trajectory = []\n",
    "\n",
    "    while not (done or truncated):\n",
    "        # Flop Counting\n",
    "        input_tensor, _ = model.policy.obs_to_tensor(obs)\n",
    "        flops = FlopCountAnalysis(model.policy, input_tensor)\n",
    "        flops.unsupported_ops_warnings(False)\n",
    "        flops = flops.total()\n",
    "        episode_flops += flops\n",
    "\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        trajectory.append((obs, action))  # Save for later render if successful\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    crashed = info.get(\"crashed\", False)\n",
    "    arrived = info.get(\"arrived\", False)\n",
    "    if crashed:\n",
    "        collisions += 1\n",
    "    if arrived:\n",
    "        destination_arrivals += 1\n",
    "    if (not crashed) and arrived:\n",
    "        success_count += 1\n",
    "        successful_flopcount += episode_flops\n",
    "        # successful_runs.append((seed, config.copy(), trajectory))\n",
    "\n",
    "    # print(f\"Episode {eps + 1} finished, total reward: {episode_reward}, destination: {config['destination']}\")\n",
    "    print(f\"Episode {eps + 1} finished, total reward: {episode_reward}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Total collisions: {collisions} out of {episodes} episodes\")\n",
    "print(f\"Total destination arrivals: {destination_arrivals} out of {episodes} episodes\")\n",
    "if success_count > 0:\n",
    "    print(f\"FLOPS per successful episode: {successful_flopcount / success_count:.2}\")\n",
    "else:\n",
    "    print(\"No successful episodes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66284c6",
   "metadata": {},
   "source": [
    "### Save Successful Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"wb\") as f:\n",
    "    pickle.dump(successful_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f032",
   "metadata": {},
   "source": [
    "### Load Successful Runs File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f3e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"rb\") as f:\n",
    "    successful_runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70751916",
   "metadata": {},
   "source": [
    "### Render Successful Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc716d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (seed, config, trajectory) in enumerate(successful_runs):\n",
    "    print(f\"\\nRendering successful episode {i + 1}\")\n",
    "    # env = gym.make(\"custom-intersection-v0\", render_mode='human', config=config)\n",
    "    env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    for obs, action in trajectory:\n",
    "        env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
