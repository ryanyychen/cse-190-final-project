{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972d8446",
   "metadata": {},
   "source": [
    "# CHANGE THESE CONFIGS, THEN UPDATE MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8810a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models/ppo/ppo\"\n",
    "TENSORBOARD_LOG_DIR = \"./models/ppo/logs\"\n",
    "IMAGE_TAG = \"ppo_rewards\"\n",
    "IMAGE_DIR = \"./images/ppo\"\n",
    "RUNS_FILE = \"./models/ppo/ppo_success_runs.pkl\"\n",
    "\n",
    "#### ENV CONFIGS ####\n",
    "CONFIG = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-10, 10],\n",
    "            \"vy\": [-10, 10],\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"clip\": False,\n",
    "        \"normalize\": False,\n",
    "    },\n",
    "    # ─────────────── Switch to continuous actions ───────────────\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "    },\n",
    "    # ───────────────────────── Other settings ─────────────────────────\n",
    "    \"duration\": 15,\n",
    "    \"simulation_frequency\": 10,\n",
    "    \"policy_frequency\": 10,\n",
    "    \"destination\": \"o1\",\n",
    "    \"initial_vehicle_count\": 20,\n",
    "    \"spawn_probability\": 0.8,\n",
    "    \"ego_spacing\": 25,\n",
    "    \"initial_lane_id\": None,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"duration\": 15,\n",
    "    \"vehicles_density\": 1.0,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.6],\n",
    "    \"scaling\": 5.5 * 1.3,\n",
    "    \"normalize_reward\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698ca84",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b59c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
    "\n",
    "from custom_intersection_env import CustomIntersectionEnv\n",
    "from simple_intersection_env import SimpleIntersectionEnv\n",
    "from custom_training_callback import RewardTrackingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33468fde",
   "metadata": {},
   "source": [
    "## Register Env with Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a76e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"simple-intersection-v0\",\n",
    "    entry_point=\"simple_intersection_env:SimpleIntersectionEnv\",\n",
    ")\n",
    "\n",
    "# gym.envs.registration.register(\n",
    "#     id=\"custom-intersection-v0\",\n",
    "#     entry_point=\"custom_intersection_env:CustomIntersectionEnv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea952aa",
   "metadata": {},
   "source": [
    "## Create and Wrap Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"simple-intersection-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "# env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d79e0f",
   "metadata": {},
   "source": [
    "## UPDATE HERE: Set Up Correct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a151beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1000`, after every 15 untruncated mini-batches, there will be a truncated mini-batch of size 40\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1000 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=1000,           # on‐policy rollout length\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.95,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    use_sde=False,          # you can set True for state‐dependent noise\n",
    "    sde_sample_freq=-1,\n",
    "    tensorboard_log=\"./ppo_tensorboard/\",\n",
    "    verbose=0,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ccb7b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030f8276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53fa5693d99454293789c9128c68952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_callback = RewardTrackingCallback(\n",
    "    tag=IMAGE_TAG,\n",
    "    path_dir=IMAGE_DIR\n",
    ")\n",
    "\n",
    "class DestinationWrapper(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.unwrapped.config[\"destination\"] = \"o\" + str(random.randint(1, 3))\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "env = gym.make(\"simple-intersection-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "# env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "env = DestinationWrapper(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model.set_env(env)  # Update the model with the new environment\n",
    "model.learn(\n",
    "    total_timesteps=100000,\n",
    "    callback=[ProgressBarCallback(), reward_callback],\n",
    "    tb_log_name=\"simple_intersection_run\"\n",
    "    # tb_log_name=\"custom_intersection_run\"\n",
    ")\n",
    "reward_callback.start_new_phase()\n",
    "reward_callback.save_all_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4906",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f66b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(MODEL_PATH)\n",
    "model.save('./models/ppo/ppo_simple_100000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578176a8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c1302",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2030a4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(MODEL_PATH)\n",
    "model = PPO.load('./models/ppo/ppo_simple_100000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb9ebe",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eeefa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 2 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 3 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 4 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 5 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 6 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 7 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 8 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 9 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 10 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 11 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 12 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 13 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 14 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 15 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 16 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 17 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 18 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 19 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 20 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 21 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 22 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 23 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 24 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 25 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 26 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 27 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 28 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 29 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 30 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 31 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 32 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 33 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 34 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 35 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 36 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 37 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 38 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 39 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 40 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 41 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 42 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 43 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 44 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 45 finished, total reward: 0, destination: o3, arrived: False, crashed: False\n",
      "Episode 46 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 47 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 48 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 49 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 50 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 51 finished, total reward: 0, destination: o3, arrived: False, crashed: False\n",
      "Episode 52 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 53 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 54 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 55 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 56 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 57 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 58 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 59 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 60 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 61 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 62 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 63 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 64 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 65 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 66 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 67 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 68 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 69 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 70 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 71 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 72 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 73 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 74 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 75 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 76 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 77 finished, total reward: -10.0, destination: o3, arrived: False, crashed: True\n",
      "Episode 78 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 79 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 80 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 81 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 82 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 83 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 84 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 85 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 86 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 87 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 88 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 89 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 90 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 91 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 92 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 93 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 94 finished, total reward: 10.0, destination: o1, arrived: True, crashed: False\n",
      "Episode 95 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 96 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Episode 97 finished, total reward: -10.0, destination: o1, arrived: False, crashed: True\n",
      "Episode 98 finished, total reward: 10.0, destination: o2, arrived: True, crashed: False\n",
      "Episode 99 finished, total reward: -10.0, destination: o2, arrived: False, crashed: True\n",
      "Episode 100 finished, total reward: 10.0, destination: o3, arrived: True, crashed: False\n",
      "Total collisions: 43 out of 100 episodes\n",
      "Total destination arrivals: 55 out of 100 episodes\n",
      "FLOPS per successful episode: 3e+06\n"
     ]
    }
   ],
   "source": [
    "import contextlib, io\n",
    "\n",
    "collisions = 0\n",
    "destination_arrivals = 0\n",
    "success_count = 0\n",
    "successful_flopcount = 0\n",
    "episodes = 100\n",
    "\n",
    "# Store successful runs for rendering\n",
    "successful_runs = []\n",
    "\n",
    "for eps in range(100):\n",
    "    config = CONFIG.copy()\n",
    "    config[\"destination\"] = \"o\" + str(random.randint(1, 3))\n",
    "    env = gym.make(\"simple-intersection-v0\", render_mode='rgb_array', config=config)\n",
    "    # env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=config)\n",
    "\n",
    "    seed = random.randint(0, 10000)\n",
    "\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    episode_flops = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    trajectory = []\n",
    "\n",
    "    while not (done or truncated):\n",
    "        # === FLOP COUNTING (silenced) ===\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "            # Anything printed by FlopCountAnalysis—whether “unused submodules” or other messages—goes into `f`\n",
    "            input_tensor, _ = model.policy.obs_to_tensor(obs)\n",
    "            flops = FlopCountAnalysis(model.policy, input_tensor)\n",
    "            flops.unsupported_ops_warnings(False)\n",
    "            flops = flops.total()\n",
    "        \n",
    "        episode_flops += flops\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        trajectory.append((obs, action))\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    crashed = info.get(\"crashed\", False)\n",
    "    arrived = info.get(\"arrived\", False)\n",
    "    if crashed:\n",
    "        collisions += 1\n",
    "    if arrived:\n",
    "        destination_arrivals += 1\n",
    "    if (not crashed) and arrived:\n",
    "        success_count += 1\n",
    "        successful_flopcount += episode_flops\n",
    "        successful_runs.append((seed, config.copy(), trajectory))\n",
    "\n",
    "    print(f\"Episode {eps + 1} finished, total reward: {episode_reward}, destination: {config['destination']}, arrived: {arrived}, crashed: {crashed}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Total collisions: {collisions} out of {episodes} episodes\")\n",
    "print(f\"Total destination arrivals: {destination_arrivals} out of {episodes} episodes\")\n",
    "if success_count > 0:\n",
    "    print(f\"FLOPS per successful episode: {successful_flopcount / success_count:.2}\")\n",
    "else:\n",
    "    print(\"No successful episodes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66284c6",
   "metadata": {},
   "source": [
    "### Save Successful Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"wb\") as f:\n",
    "    pickle.dump(successful_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f032",
   "metadata": {},
   "source": [
    "### Load Successful Runs File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66f3e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"rb\") as f:\n",
    "    successful_runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70751916",
   "metadata": {},
   "source": [
    "### Render Successful Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc716d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rendering successful episode 1\n",
      "\n",
      "Rendering successful episode 2\n",
      "\n",
      "Rendering successful episode 3\n",
      "\n",
      "Rendering successful episode 4\n",
      "\n",
      "Rendering successful episode 5\n",
      "\n",
      "Rendering successful episode 6\n",
      "\n",
      "Rendering successful episode 7\n",
      "\n",
      "Rendering successful episode 8\n",
      "\n",
      "Rendering successful episode 9\n",
      "\n",
      "Rendering successful episode 10\n",
      "\n",
      "Rendering successful episode 11\n",
      "\n",
      "Rendering successful episode 12\n",
      "\n",
      "Rendering successful episode 13\n",
      "\n",
      "Rendering successful episode 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     env.step(action)\n\u001b[32m      7\u001b[39m     env.render()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m env.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, (seed, config, trajectory) in enumerate(successful_runs[20:]):\n",
    "    print(f\"\\nRendering successful episode {i + 1}\")\n",
    "    env = gym.make(\"simple-intersection-v0\", render_mode='human', config=config)\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    for obs, action in trajectory:\n",
    "        env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
