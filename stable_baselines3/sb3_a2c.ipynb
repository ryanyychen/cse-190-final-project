{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972d8446",
   "metadata": {},
   "source": [
    "# CHANGE THESE CONFIGS, THEN UPDATE MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8810a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models/a2c/a2c\"\n",
    "TENSORBOARD_LOG_DIR = \"./models/a2c/logs\"\n",
    "IMAGE_TAG = \"a2c_rewards\"\n",
    "IMAGE_DIR = \"./images/a2c\"\n",
    "RUNS_FILE = \"./models/a2c/a2c_success_runs.pkl\"\n",
    "\n",
    "#### ENV CONFIGS ####\n",
    "CONFIG = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,  # Number of other vehicles to observe\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],  # Observe position and velocity\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-10, 10],\n",
    "            \"vy\": [-10, 10]\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"clip\": False,\n",
    "        \"normalize\": False\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",  # Keep simple, 5 discrete actions\n",
    "    },\n",
    "    \"simulation_frequency\": 10,\n",
    "    \"policy_frequency\": 10,\n",
    "    \"destination\": \"o1\",\n",
    "    \"initial_vehicle_count\": 20,\n",
    "    \"spawn_probability\": 0.8,\n",
    "    \"ego_spacing\": 25,\n",
    "    \"initial_lane_id\": None,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"duration\": 15,  # seconds\n",
    "    \"vehicles_density\": 1.0,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.6],\n",
    "    \"scaling\": 5.5 * 1.3,\n",
    "    \"normalize_reward\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698ca84",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b59c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
    "\n",
    "from custom_intersection_env import CustomIntersectionEnv\n",
    "from custom_training_callback import RewardTrackingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33468fde",
   "metadata": {},
   "source": [
    "## Register Env with Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a76e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id='custom-intersection-v0',\n",
    "    entry_point='custom_intersection_env:CustomIntersectionEnv',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea952aa",
   "metadata": {},
   "source": [
    "## Create and Wrap Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=CONFIG)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d79e0f",
   "metadata": {},
   "source": [
    "## UPDATE HERE: Set Up Correct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a151beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    n_steps=5,\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    rms_prop_eps=1e-5,\n",
    "    use_rms_prop=True,\n",
    "    normalize_advantage=False,\n",
    "    tensorboard_log=TENSORBOARD_LOG_DIR,\n",
    "    verbose=0,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ccb7b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030f8276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "be0229b509e64eb78d7feae87f03e083",
=======
       "model_id": "f5ddc311ba804e9fa8d334ec29663f6e",
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "4c77220a9ca54860b288ab1d71aa1522",
=======
       "model_id": "5c59c60de0f34a3a8a97bfaea4b722e0",
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "bb6ba4f392fa4fe88dc9aae308fa7556",
=======
       "model_id": "8139d5fb48784ed08f06b9424a6e9e32",
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_callback = RewardTrackingCallback(\n",
    "    tag=IMAGE_TAG,\n",
    "    path_dir=IMAGE_DIR\n",
    ")\n",
    "\n",
    "destinations = [\"o1\", \"o2\", \"o3\"]\n",
    "steps = [50000 for _ in destinations]\n",
    "\n",
    "for dest, steps in zip(destinations, steps):\n",
    "    config = CONFIG.copy()\n",
    "    config[\"destination\"] = dest  # Change destination for each training phase\n",
    "    env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=config)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    model.set_env(env)  # Update the model with the new environment\n",
    "    model.learn(\n",
    "        total_timesteps=steps,\n",
    "        callback=[ProgressBarCallback(), reward_callback]\n",
    "    )\n",
    "    reward_callback.start_new_phase()\n",
    "reward_callback.save_all_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c4906",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f66b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578176a8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c1302",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2030a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb9ebe",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eeefa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Episode 1 finished, total reward: 459.18286782358285, destination: o3\n",
      "Episode 2 finished, total reward: 84.87909871544858, destination: o2\n",
      "Episode 3 finished, total reward: 105.10089711997873, destination: o3\n",
      "Episode 4 finished, total reward: 237.0373454133218, destination: o2\n",
      "Episode 5 finished, total reward: 571.6252453077983, destination: o3\n",
      "Episode 6 finished, total reward: 340.09673432631774, destination: o2\n",
      "Episode 7 finished, total reward: 385.4358538318417, destination: o1\n",
      "Episode 8 finished, total reward: 474.4782986947313, destination: o1\n",
      "Episode 9 finished, total reward: 296.64548904374266, destination: o1\n",
      "Episode 10 finished, total reward: 597.9870045947154, destination: o3\n",
      "Episode 11 finished, total reward: 186.0586327028722, destination: o1\n",
      "Episode 12 finished, total reward: 120.34017673609051, destination: o1\n",
      "Episode 13 finished, total reward: 570.4486468430251, destination: o3\n",
      "Episode 14 finished, total reward: 159.41867918227825, destination: o1\n",
      "Episode 15 finished, total reward: 206.69701814176125, destination: o2\n",
      "Episode 16 finished, total reward: 257.90940201292415, destination: o2\n",
      "Episode 17 finished, total reward: 495.9222593527022, destination: o3\n",
      "Episode 18 finished, total reward: 555.7617387947496, destination: o1\n",
      "Episode 19 finished, total reward: 628.6919583814544, destination: o3\n",
      "Episode 20 finished, total reward: 461.70322359719694, destination: o3\n",
      "Episode 21 finished, total reward: 266.6810297469168, destination: o1\n",
      "Episode 22 finished, total reward: 125.06468638473717, destination: o2\n",
      "Episode 23 finished, total reward: 435.6428261510847, destination: o1\n",
      "Episode 24 finished, total reward: 163.93536330454063, destination: o2\n",
      "Episode 25 finished, total reward: 191.2800329226831, destination: o2\n",
      "Episode 26 finished, total reward: 79.66117469410955, destination: o2\n",
      "Episode 27 finished, total reward: 508.5975058840642, destination: o1\n",
      "Episode 28 finished, total reward: 116.50641275647854, destination: o1\n",
      "Episode 29 finished, total reward: 163.72048378638002, destination: o2\n",
      "Episode 30 finished, total reward: 377.128732120847, destination: o3\n",
      "Episode 31 finished, total reward: 416.86907389096245, destination: o1\n",
      "Episode 32 finished, total reward: 193.44980650778197, destination: o2\n",
      "Episode 33 finished, total reward: 215.9874957157256, destination: o1\n",
      "Episode 34 finished, total reward: 148.3099107396672, destination: o1\n",
      "Episode 35 finished, total reward: 108.87402031837571, destination: o1\n",
      "Episode 36 finished, total reward: 712.5977288987174, destination: o1\n",
      "Episode 37 finished, total reward: 1500.036811780284, destination: o1\n",
      "Episode 38 finished, total reward: 555.8300288212668, destination: o1\n",
      "Episode 39 finished, total reward: 412.1848593490038, destination: o1\n",
      "Episode 40 finished, total reward: 119.16249153742783, destination: o1\n",
      "Episode 41 finished, total reward: 257.60892304681545, destination: o1\n",
      "Episode 42 finished, total reward: 710.9297084141882, destination: o1\n",
      "Episode 43 finished, total reward: 547.7782534656357, destination: o3\n",
      "Episode 44 finished, total reward: 606.4615439998225, destination: o3\n",
      "Episode 45 finished, total reward: 497.1675963662047, destination: o3\n",
      "Episode 46 finished, total reward: 318.3522965322371, destination: o2\n",
      "Episode 47 finished, total reward: 315.79522522096056, destination: o1\n",
      "Episode 48 finished, total reward: 427.3194835013945, destination: o2\n",
      "Episode 49 finished, total reward: 287.5126081641014, destination: o2\n",
      "Episode 50 finished, total reward: 60.15975170116824, destination: o3\n",
      "Episode 51 finished, total reward: 244.23750417319897, destination: o2\n",
      "Episode 52 finished, total reward: 510.6960751884212, destination: o3\n",
      "Episode 53 finished, total reward: 202.77953580416263, destination: o2\n",
      "Episode 54 finished, total reward: 533.5820565619723, destination: o3\n",
      "Episode 55 finished, total reward: 246.30993483097674, destination: o1\n",
      "Episode 56 finished, total reward: 181.62823940435658, destination: o1\n",
      "Episode 57 finished, total reward: 686.210081092487, destination: o1\n",
      "Episode 58 finished, total reward: 331.76779079291464, destination: o2\n",
      "Episode 59 finished, total reward: 341.9775118388723, destination: o2\n",
      "Episode 60 finished, total reward: 103.98172954519225, destination: o1\n",
      "Episode 61 finished, total reward: 562.3730690871931, destination: o3\n",
      "Episode 62 finished, total reward: 91.45594822131415, destination: o2\n",
      "Episode 63 finished, total reward: 142.5418535693258, destination: o2\n",
      "Episode 64 finished, total reward: 147.87021081509545, destination: o2\n",
      "Episode 65 finished, total reward: 590.189271922405, destination: o3\n",
      "Episode 66 finished, total reward: 124.1780050225039, destination: o2\n",
      "Episode 67 finished, total reward: 109.13772171957248, destination: o2\n",
      "Episode 68 finished, total reward: 146.6620456993682, destination: o1\n",
      "Episode 69 finished, total reward: 208.8517515478656, destination: o2\n",
      "Episode 70 finished, total reward: 212.85556300731244, destination: o1\n",
      "Episode 71 finished, total reward: 198.5530294613491, destination: o1\n",
      "Episode 72 finished, total reward: 243.54517969444564, destination: o3\n",
      "Episode 73 finished, total reward: 599.9245183475256, destination: o3\n",
      "Episode 74 finished, total reward: 148.79797290801142, destination: o1\n",
      "Episode 75 finished, total reward: 196.6076652112298, destination: o2\n",
      "Episode 76 finished, total reward: 454.9355864093768, destination: o1\n",
      "Episode 77 finished, total reward: 171.70048613614978, destination: o1\n",
      "Episode 78 finished, total reward: 607.4525036058643, destination: o3\n",
      "Episode 79 finished, total reward: 314.51108763565844, destination: o2\n",
      "Episode 80 finished, total reward: 72.88971192023507, destination: o3\n",
      "Episode 81 finished, total reward: 511.7164206344831, destination: o3\n",
      "Episode 82 finished, total reward: 242.99197492758205, destination: o2\n",
      "Episode 83 finished, total reward: 260.91271256305527, destination: o1\n",
      "Episode 84 finished, total reward: 622.1932634312193, destination: o3\n",
      "Episode 85 finished, total reward: 428.7644808069939, destination: o1\n",
      "Episode 86 finished, total reward: 240.6649998796946, destination: o2\n",
      "Episode 87 finished, total reward: 302.2404029762325, destination: o1\n",
      "Episode 88 finished, total reward: 109.82138344533094, destination: o3\n",
      "Episode 89 finished, total reward: 545.027936178295, destination: o3\n",
      "Episode 90 finished, total reward: 555.884470213152, destination: o3\n",
      "Episode 91 finished, total reward: 435.5648961836746, destination: o3\n",
      "Episode 92 finished, total reward: 180.72153609794307, destination: o1\n",
      "Episode 93 finished, total reward: 260.897857147452, destination: o2\n",
      "Episode 94 finished, total reward: 576.3241469957052, destination: o1\n",
      "Episode 95 finished, total reward: 233.1548081601464, destination: o1\n",
      "Episode 96 finished, total reward: 332.01648300948483, destination: o2\n",
      "Episode 97 finished, total reward: 140.9593391493395, destination: o1\n",
      "Episode 98 finished, total reward: 213.4838505390995, destination: o1\n",
      "Episode 99 finished, total reward: 224.89201612404554, destination: o2\n",
      "Episode 100 finished, total reward: 364.8542935554161, destination: o2\n",
      "Total collisions: 73 out of 100 episodes\n",
      "Total destination arrivals: 19 out of 100 episodes\n",
      "FLOPS per successful episode: 3.8e+06\n"
=======
      "Episode 1 finished, total reward: -25.0, destination: o2\n",
      "Episode 2 finished, total reward: -25.0, destination: o1\n",
      "Episode 3 finished, total reward: -25.0, destination: o3\n",
      "Episode 4 finished, total reward: -25.0, destination: o2\n",
      "Episode 5 finished, total reward: 50.0, destination: o2\n",
      "Episode 6 finished, total reward: -25.0, destination: o3\n",
      "Episode 7 finished, total reward: -25.0, destination: o1\n",
      "Episode 8 finished, total reward: -25.0, destination: o1\n",
      "Episode 9 finished, total reward: -25.0, destination: o2\n",
      "Episode 10 finished, total reward: 0.0, destination: o1\n",
      "Episode 11 finished, total reward: -25.0, destination: o3\n",
      "Episode 12 finished, total reward: -50.0, destination: o2\n",
      "Episode 13 finished, total reward: -25.0, destination: o1\n",
      "Episode 14 finished, total reward: -25.0, destination: o1\n",
      "Episode 15 finished, total reward: -25.0, destination: o3\n",
      "Episode 16 finished, total reward: -25.0, destination: o3\n",
      "Episode 17 finished, total reward: -25.0, destination: o1\n",
      "Episode 18 finished, total reward: -25.0, destination: o2\n",
      "Episode 19 finished, total reward: -25.0, destination: o1\n",
      "Episode 20 finished, total reward: 50.0, destination: o2\n",
      "Episode 21 finished, total reward: 50.0, destination: o2\n",
      "Episode 22 finished, total reward: -25.0, destination: o1\n",
      "Episode 23 finished, total reward: -25.0, destination: o2\n",
      "Episode 24 finished, total reward: 50.0, destination: o3\n",
      "Episode 25 finished, total reward: 50.0, destination: o3\n",
      "Episode 26 finished, total reward: 50.0, destination: o3\n",
      "Episode 27 finished, total reward: -25.0, destination: o1\n",
      "Episode 28 finished, total reward: -25.0, destination: o1\n",
      "Episode 29 finished, total reward: -25.0, destination: o3\n",
      "Episode 30 finished, total reward: -25.0, destination: o1\n",
      "Episode 31 finished, total reward: -25.0, destination: o1\n",
      "Episode 32 finished, total reward: -25.0, destination: o1\n",
      "Episode 33 finished, total reward: -25.0, destination: o2\n",
      "Episode 34 finished, total reward: -25.0, destination: o3\n",
      "Episode 35 finished, total reward: 50.0, destination: o3\n",
      "Episode 36 finished, total reward: -25.0, destination: o1\n",
      "Episode 37 finished, total reward: -25.0, destination: o1\n",
      "Episode 38 finished, total reward: 50.0, destination: o2\n",
      "Episode 39 finished, total reward: -25.0, destination: o1\n",
      "Episode 40 finished, total reward: -25.0, destination: o2\n",
      "Episode 41 finished, total reward: -25.0, destination: o3\n",
      "Episode 42 finished, total reward: -25.0, destination: o3\n",
      "Episode 43 finished, total reward: -25.0, destination: o1\n",
      "Episode 44 finished, total reward: -25.0, destination: o1\n",
      "Episode 45 finished, total reward: -25.0, destination: o3\n",
      "Episode 46 finished, total reward: -25.0, destination: o1\n",
      "Episode 47 finished, total reward: -25.0, destination: o2\n",
      "Episode 48 finished, total reward: -25.0, destination: o2\n",
      "Episode 49 finished, total reward: -25.0, destination: o3\n",
      "Episode 50 finished, total reward: -25.0, destination: o2\n",
      "Episode 51 finished, total reward: -25.0, destination: o2\n",
      "Episode 52 finished, total reward: 50.0, destination: o3\n",
      "Episode 53 finished, total reward: -25.0, destination: o1\n",
      "Episode 54 finished, total reward: -25.0, destination: o2\n",
      "Episode 55 finished, total reward: -25.0, destination: o1\n",
      "Episode 56 finished, total reward: -25.0, destination: o2\n",
      "Episode 57 finished, total reward: -25.0, destination: o2\n",
      "Episode 58 finished, total reward: -25.0, destination: o1\n",
      "Episode 59 finished, total reward: -25.0, destination: o1\n",
      "Episode 60 finished, total reward: -25.0, destination: o1\n",
      "Episode 61 finished, total reward: -25.0, destination: o2\n",
      "Episode 62 finished, total reward: -25.0, destination: o3\n",
      "Episode 63 finished, total reward: -25.0, destination: o1\n",
      "Episode 64 finished, total reward: -25.0, destination: o2\n",
      "Episode 65 finished, total reward: -25.0, destination: o1\n",
      "Episode 66 finished, total reward: -25.0, destination: o2\n",
      "Episode 67 finished, total reward: -25.0, destination: o3\n",
      "Episode 68 finished, total reward: -25.0, destination: o3\n",
      "Episode 69 finished, total reward: -25.0, destination: o1\n",
      "Episode 70 finished, total reward: 50.0, destination: o3\n",
      "Episode 71 finished, total reward: -25.0, destination: o1\n",
      "Episode 72 finished, total reward: 50.0, destination: o2\n",
      "Episode 73 finished, total reward: 50.0, destination: o3\n",
      "Episode 74 finished, total reward: -25.0, destination: o1\n",
      "Episode 75 finished, total reward: -25.0, destination: o3\n",
      "Episode 76 finished, total reward: -25.0, destination: o3\n",
      "Episode 77 finished, total reward: -25.0, destination: o1\n",
      "Episode 78 finished, total reward: -25.0, destination: o2\n",
      "Episode 79 finished, total reward: -25.0, destination: o1\n",
      "Episode 80 finished, total reward: -25.0, destination: o1\n",
      "Episode 81 finished, total reward: -25.0, destination: o3\n",
      "Episode 82 finished, total reward: -25.0, destination: o3\n",
      "Episode 83 finished, total reward: -25.0, destination: o1\n",
      "Episode 84 finished, total reward: -25.0, destination: o2\n",
      "Episode 85 finished, total reward: 50.0, destination: o2\n",
      "Episode 86 finished, total reward: -25.0, destination: o3\n",
      "Episode 87 finished, total reward: -25.0, destination: o2\n",
      "Episode 88 finished, total reward: -25.0, destination: o2\n",
      "Episode 89 finished, total reward: -25.0, destination: o3\n",
      "Episode 90 finished, total reward: -25.0, destination: o1\n",
      "Episode 91 finished, total reward: -25.0, destination: o2\n",
      "Episode 92 finished, total reward: -25.0, destination: o2\n",
      "Episode 93 finished, total reward: -25.0, destination: o2\n",
      "Episode 94 finished, total reward: 50.0, destination: o3\n",
      "Episode 95 finished, total reward: -25.0, destination: o1\n",
      "Episode 96 finished, total reward: -25.0, destination: o2\n",
      "Episode 97 finished, total reward: -25.0, destination: o2\n",
      "Episode 98 finished, total reward: 50.0, destination: o3\n",
      "Episode 99 finished, total reward: -25.0, destination: o1\n",
      "Episode 100 finished, total reward: -25.0, destination: o2\n",
      "Total collisions: 84 out of 100 episodes\n",
      "Total destination arrivals: 15 out of 100 episodes\n",
      "FLOPS per successful episode: 1.3e+06\n"
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
     ]
    }
   ],
   "source": [
    "collisions = 0\n",
    "destination_arrivals = 0\n",
    "success_count = 0\n",
    "successful_flopcount = 0\n",
    "episodes = 100\n",
    "\n",
    "# Store successful runs for rendering\n",
    "successful_runs = []\n",
    "\n",
    "for eps in range(100):\n",
    "    config = CONFIG.copy()\n",
    "    config[\"destination\"] = \"o\" + str(random.randint(1, 3))\n",
    "    env = gym.make(\"custom-intersection-v0\", render_mode='rgb_array', config=config)\n",
    "\n",
<<<<<<< HEAD
    "    seed = random.randint(0, 10000)\n",
=======
    "    seed = random.randint(0, 10000())\n",
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
    "\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    episode_flops = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    trajectory = []\n",
    "\n",
    "    while not (done or truncated):\n",
    "        # Flop Counting\n",
    "        input_tensor, _ = model.policy.obs_to_tensor(obs)\n",
    "        flops = FlopCountAnalysis(model.policy, input_tensor)\n",
    "        flops.unsupported_ops_warnings(False)\n",
    "        flops = flops.total()\n",
    "        episode_flops += flops\n",
    "\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        trajectory.append((obs, action))  # Save for later render if successful\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    crashed = info.get(\"crashed\", False)\n",
    "    arrived = info.get(\"arrived\", False)\n",
    "    if crashed:\n",
    "        collisions += 1\n",
    "    if arrived:\n",
    "        destination_arrivals += 1\n",
    "    if (not crashed) and arrived:\n",
    "        success_count += 1\n",
    "        successful_flopcount += episode_flops\n",
    "        successful_runs.append((seed, config.copy(), trajectory))\n",
    "\n",
    "    print(f\"Episode {eps + 1} finished, total reward: {episode_reward}, destination: {config['destination']}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Total collisions: {collisions} out of {episodes} episodes\")\n",
    "print(f\"Total destination arrivals: {destination_arrivals} out of {episodes} episodes\")\n",
    "if success_count > 0:\n",
    "    print(f\"FLOPS per successful episode: {successful_flopcount / success_count:.2}\")\n",
    "else:\n",
    "    print(\"No successful episodes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66284c6",
   "metadata": {},
   "source": [
    "### Save Successful Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c84cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"wb\") as f:\n",
    "    pickle.dump(successful_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f032",
   "metadata": {},
   "source": [
    "### Load Successful Runs File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f3e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RUNS_FILE, \"rb\") as f:\n",
    "    successful_runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70751916",
   "metadata": {},
   "source": [
    "### Render Successful Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc716d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rendering successful episode 1\n",
      "\n",
<<<<<<< HEAD
      "Rendering successful episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:09:21.744 python[26214:2873478] error messaging the mach port for IMKCFRunLoopWakeUpReliable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
=======
      "Rendering successful episode 2\n",
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
      "\n",
      "Rendering successful episode 3\n",
      "\n",
      "Rendering successful episode 4\n",
      "\n",
      "Rendering successful episode 5\n",
      "\n",
      "Rendering successful episode 6\n",
      "\n",
      "Rendering successful episode 7\n",
      "\n",
      "Rendering successful episode 8\n",
      "\n",
      "Rendering successful episode 9\n",
      "\n",
      "Rendering successful episode 10\n",
      "\n",
      "Rendering successful episode 11\n",
      "\n",
      "Rendering successful episode 12\n",
      "\n",
      "Rendering successful episode 13\n",
      "\n",
      "Rendering successful episode 14\n",
      "\n",
<<<<<<< HEAD
      "Rendering successful episode 15\n",
      "\n",
      "Rendering successful episode 16\n",
      "\n",
      "Rendering successful episode 17\n",
      "\n",
      "Rendering successful episode 18\n",
      "\n",
      "Rendering successful episode 19\n"
=======
      "Rendering successful episode 15\n"
>>>>>>> 5f5f4463 (Add seeding for replicability during eval and render)
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, (seed, config, trajectory) in enumerate(successful_runs):\n",
    "    print(f\"\\nRendering successful episode {i + 1}\")\n",
    "    env = gym.make(\"custom-intersection-v0\", render_mode='human', config=config)\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    for obs, action in trajectory:\n",
    "        env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
