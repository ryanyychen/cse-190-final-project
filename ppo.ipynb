{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74837bb",
   "metadata": {},
   "source": [
    "# PPO AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba591bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preranagowda/Desktop/cse-190-final-project/venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# === 1) Imports ===\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from env import create_env\n",
    "from algorithms.ppo import PPOAgent\n",
    "\n",
    "# If you want progress bars:\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d628384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment config: {'env': 'intersection-v0', 'config': {'duration': 50, 'simulation_frequency': 15, 'policy_frequency': 2, 'destination': 'o1', 'initial_vehicle_count': 10, 'spawn_probability': 0.6, 'observation': {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True}, 'vehicle': {'acceleration': 3.0, 'steering': 0.4}, 'action': {'type': 'ContinuousAction'}, 'high_speed_reward': 0.0, 'collision_reward': -100.0, 'arrived_reward': 50.0, 'reward_speed_range': [0.0, 3.0], 'normalize_reward': False, 'collision_terminal': True, 'offroad_terminal': True}, 'wrapper_config': {'steer_factor': 2.0, 'speed_factor': 5.0, 'onroad_reward': 5.0, 'progress_reward': 5.0, 'wrongexit_penalty': 20.0, 'offroad_penalty': 20.0, 'collision_penalty': 100.0, 'collision_terminal': True, 'offroad_terminal': True}}\n",
      "Loaded PPO config: {'hidden_size': 128, 'learning_rate': 0.0003, 'gamma': 0.99, 'clip_epsilon': 0.2, 'k_epochs': 4, 'gae_lambda': 0.95, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'update_timestep': 4000, 'model_path': 'models/ppo.pth', 'num_episodes': 2000, 'print_freq': 100, 'save_freq': 100, 'recorder': True}\n"
     ]
    }
   ],
   "source": [
    "# === 2) Load configs ===\n",
    "ENV_CONFIG   = \"./configs/env.yaml\"\n",
    "MODEL_CONFIG   = \"./configs/ppo.yaml\"\n",
    "\n",
    "with open(ENV_CONFIG, \"r\") as f_env:\n",
    "    env_cfg = yaml.safe_load(f_env)\n",
    "\n",
    "with open(MODEL_CONFIG, \"r\") as f_ppo:\n",
    "    ppo_cfg = yaml.safe_load(f_ppo)\n",
    "\n",
    "print(\"Loaded environment config:\", env_cfg)\n",
    "print(\"Loaded PPO config:\", ppo_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b431c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.0000000e+00,  2.0000000e-02,  4.6569738e-01,  0.0000000e+00,\n",
       "         -5.0000000e-01,  6.1232343e-17, -1.0000000e+00],\n",
       "        [ 1.0000000e+00, -2.3217291e-01, -4.4569737e-01,  3.7858361e-01,\n",
       "          5.0000000e-01,  1.0000000e+00, -2.0576134e-16],\n",
       "        [ 1.0000000e+00, -4.8081377e-01, -4.4569737e-01,  4.0000001e-01,\n",
       "          5.0000000e-01,  1.0000000e+00, -5.7731595e-17],\n",
       "        [ 1.0000000e+00,  7.8728282e-01, -4.8569736e-01, -3.1105015e-01,\n",
       "          5.0000000e-01, -1.0000000e+00,  1.2246469e-16],\n",
       "        [ 1.0000000e+00,  5.0120562e-01, -4.8569736e-01, -4.3888959e-01,\n",
       "          5.0000000e-01, -1.0000000e+00,  1.2246469e-16],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32),\n",
       " {'speed': 10,\n",
       "  'crashed': False,\n",
       "  'action': array([0.45382738, 0.22934076], dtype=float32),\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'high_speed_reward': np.float64(1.0),\n",
       "   'arrived_reward': 0.0,\n",
       "   'on_road_reward': np.float64(1.0)},\n",
       "  'agents_rewards': (np.float64(0.0),),\n",
       "  'agents_terminated': (False,)})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 3) Create the Gym/Highway environment ===\n",
    "\n",
    "env = create_env(config_filepath=ENV_CONFIG, render_mode=None)\n",
    "# Some envs (like highway-env) ignore render_mode=None and only do rendering if you call env.render().\n",
    "\n",
    "# (Optional) set seed\n",
    "SEED = 42\n",
    "env.reset(seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0cb785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 70, Action size: 2\n"
     ]
    }
   ],
   "source": [
    "# === 4) Figure out state_size and action_size exactly as you did for REINFORCE ===\n",
    "# For continuous‐action highway-env, observation_space.shape might be something like (5,) or (84,84,3) if you chose image.\n",
    "obs_shape  = env.observation_space.shape\n",
    "state_size = int(np.prod(obs_shape))         # flatten everything\n",
    "action_size= int(env.action_space.shape[0])  # # of continuous dims: e.g. [accel, steering]\n",
    "\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29cb8042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs‐space: Box(-inf, inf, (10, 7), float32)\n",
      "One sample state: [[-0.8285017   1.5437019  -0.6186735   1.4928973   0.77766263 -0.03846077\n",
      "  -0.75634974]\n",
      " [-0.76338345  0.8503617  -0.4950546   0.6391455  -1.7446786  -0.3492151\n",
      "  -0.88039607]\n",
      " [-0.6594557   0.08536605 -0.6350263  -0.74557966 -0.8318835  -0.0448039\n",
      "  -0.2021981 ]\n",
      " [-0.92255825 -0.75861406 -2.1740499   0.8802994  -0.67659503  0.46809664\n",
      "  -0.74759203]\n",
      " [-0.38900954 -0.3746088  -0.24088493 -1.4239712  -0.01143532  0.7074346\n",
      "   0.26056388]\n",
      " [-0.04506813  0.9396974  -0.7119521   0.5658825  -1.5593926  -1.871649\n",
      "   0.8139259 ]\n",
      " [-1.2227253  -0.5659393   0.04407877  0.8774446  -0.1609159  -0.84062713\n",
      "   1.0519067 ]\n",
      " [-1.3404357   1.0662632   0.273187    0.6322325   0.53295463 -1.4123174\n",
      "   0.3152672 ]\n",
      " [-1.9568832  -0.7246894   0.19417413 -0.28590465  0.4958102  -0.09689894\n",
      "  -0.7736633 ]\n",
      " [-0.34719574  1.6895012  -1.8053689   1.0755097  -0.46364075 -0.4984408\n",
      "  -0.34496853]]\n",
      "sample_state.shape → (10, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Obs‐space:\", env.observation_space)        # e.g. Box(low=…, high=…, shape=(6,), dtype=float32)\n",
    "# Grab a single observation\n",
    "sample_state = env.observation_space.sample()      # this is a NumPy array of shape (6,)\n",
    "print(\"One sample state:\", sample_state)\n",
    "print(\"sample_state.shape →\", np.array(sample_state).shape)\n",
    "\n",
    "\n",
    "\n",
    "# obs_shape  = env.observation_space.shape      # → (10, 7)\n",
    "# state_size = int(np.prod(obs_shape))          # → 10*7 = 70\n",
    "# print(\"Using state_size =\", state_size)       # should print “Using state_size = 70”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c52fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) Instantiate PPOAgent ===\n",
    "#    Match the names in your ppo.py __init__ signature.\n",
    "agent = PPOAgent(\n",
    "    state_size      = state_size,          \n",
    "    hidden_size     = ppo_cfg[\"hidden_size\"],\n",
    "    action_size     = action_size,\n",
    "    learning_rate   = ppo_cfg[\"learning_rate\"],\n",
    "    gamma           = ppo_cfg[\"gamma\"],\n",
    "    clip_epsilon    = ppo_cfg[\"clip_epsilon\"],\n",
    "    k_epochs        = ppo_cfg[\"k_epochs\"],\n",
    "    gae_lambda      = ppo_cfg.get(\"gae_lambda\", 0.95),\n",
    "    entropy_coef    = ppo_cfg[\"entropy_coef\"],\n",
    "    value_loss_coef = ppo_cfg[\"value_loss_coef\"],\n",
    "    max_grad_norm   = ppo_cfg.get(\"max_grad_norm\", 0.5),\n",
    "    update_timestep = ppo_cfg[\"update_timestep\"],\n",
    "    model_path      = ppo_cfg[\"model_path\"]\n",
    ")\n",
    "\n",
    "\n",
    "# (Optional) If you have a pretrained PPO model you want to load:\n",
    "# agent.load_model(ppo_cfg[\"model_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "176120c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO Agent:   0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 10 at dim 1 (got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m print_freq      = ppo_cfg[\u001b[33m\"\u001b[39m\u001b[33mprint_freq\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m save_freq       = ppo_cfg[\u001b[33m\"\u001b[39m\u001b[33msave_freq\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_timestep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_timestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_freq\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cse-190-final-project/algorithms/ppo.py:237\u001b[39m, in \u001b[36mPPOAgent.train\u001b[39m\u001b[34m(self, env, num_episodes, update_timestep, print_freq, save_freq)\u001b[39m\n\u001b[32m    233\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Select action (tanhed and scaled), get log_prob and value\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     action_tanh, log_prob, value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Convert action to numpy and scale to environment's action space\u001b[39;00m\n\u001b[32m    240\u001b[39m     action_np = action_tanh.detach().cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cse-190-final-project/algorithms/ppo.py:110\u001b[39m, in \u001b[36mPPOAgent.select_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Given a state (numpy array), returns:\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m      - action (tensor, shape [action_size])\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m      - log_prob (tensor scalar)\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m      - value (tensor scalar)\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     state_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m.flatten().unsqueeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    111\u001b[39m     mean, log_std, value = \u001b[38;5;28mself\u001b[39m.policy_value_net(state_tensor)\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Replace NaNs just in case\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 10 at dim 1 (got 6)"
     ]
    }
   ],
   "source": [
    "# === 6) Call train(...) ===\n",
    "#    Notice that PPOAgent.train takes an extra argument update_timestep.\n",
    "num_episodes    = ppo_cfg[\"num_episodes\"]\n",
    "update_timestep = ppo_cfg[\"update_timestep\"]\n",
    "print_freq      = ppo_cfg[\"print_freq\"]\n",
    "save_freq       = ppo_cfg[\"save_freq\"]\n",
    "\n",
    "agent.train(\n",
    "    env,\n",
    "    num_episodes    = num_episodes,\n",
    "    update_timestep = update_timestep,\n",
    "    print_freq      = print_freq,\n",
    "    save_freq       = save_freq\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7) (Optional) After training, close the env or save final model ===\n",
    "agent.save_model(ppo_cfg[\"model_path\"])\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
