{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dda4c5",
   "metadata": {},
   "source": [
    "# REINFORCE (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-cell script to run evaluation\n",
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "MODEL_PATH = './models/reinforce_ep3000.pth'\n",
    "\n",
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "print(\"Environment configuration:\")\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')\n",
    "\n",
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    print(\"Model configuration:\")\n",
    "    print(config)\n",
    "\n",
    "state_size = np.prod(eval_env.observation_space.shape)\n",
    "action_size = eval_env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.load_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ad35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "MODEL_PATH = './models/reinforce.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5855faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True}\n",
      "action: {'type': 'ContinuousAction'}\n",
      "simulation_frequency: 15\n",
      "policy_frequency: 2\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 50\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -100.0\n",
      "high_speed_reward: 0.0\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: False\n",
      "offroad_terminal: True\n",
      "vehicle: {'acceleration': 3.0, 'steering': 0.4}\n",
      "collision_terminal: True\n"
     ]
    }
   ],
   "source": [
    "env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode=None,\n",
    ")\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in env.config.keys():\n",
    "    print(f'{key}: {env.config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9723e3d",
   "metadata": {},
   "source": [
    "## Load Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b824c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 64\n",
      "learning_rate: 0.001\n",
      "gamma: 0.8\n",
      "num_episodes_train: 3000\n",
      "print_freq: 100\n",
      "save_freq: 1000\n",
      "num_episodes_eval: 100\n",
      "top_k: 5\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "for key in config.keys():\n",
    "    print(f'{key}: {config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94399219",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f9e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 70, Action size: 2\n"
     ]
    }
   ],
   "source": [
    "state_size = np.prod(env.observation_space.shape)\n",
    "action_size = env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6572a8",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d96ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   0%|          | 1/3000 [00:00<08:23,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 2.00 at episode 1\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   1%|▏         | 38/3000 [00:04<05:46,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 3.00 at episode 37\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   3%|▎         | 101/3000 [00:11<06:31,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/3000 | Max reward: 3.00 | Avg reward: 1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▍         | 139/3000 [00:16<05:33,  8.59it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:   5%|▍         | 143/3000 [00:17<12:40,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 4.00 at episode 143\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▍         | 145/3000 [00:19<23:43,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 19.00 at episode 145\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   7%|▋         | 200/3000 [00:54<32:12,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/3000 | Max reward: 19.00 | Avg reward: 2.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   7%|▋         | 213/3000 [01:02<26:06,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 20.00 at episode 212\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  10%|▉         | 289/3000 [01:56<44:51,  1.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 21.00 at episode 289\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  10%|▉         | 290/3000 [01:57<41:15,  1.09it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  10%|█         | 300/3000 [02:04<25:31,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/3000 | Max reward: 21.00 | Avg reward: 4.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  13%|█▎        | 400/3000 [03:15<30:41,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400/3000 | Max reward: 21.00 | Avg reward: 5.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  14%|█▍        | 434/3000 [03:40<32:34,  1.31it/s]  /Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  17%|█▋        | 500/3000 [04:22<21:27,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/3000 | Max reward: 21.00 | Avg reward: 5.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  19%|█▊        | 560/3000 [05:07<39:15,  1.04it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  20%|█▉        | 593/3000 [05:30<22:25,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 22.00 at episode 592\n",
      "Model saved to ./models/reinforce.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  20%|██        | 600/3000 [05:34<19:51,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600/3000 | Max reward: 22.00 | Avg reward: 5.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  23%|██▎       | 700/3000 [06:38<26:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700/3000 | Max reward: 22.00 | Avg reward: 5.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  27%|██▋       | 800/3000 [07:45<31:18,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/3000 | Max reward: 22.00 | Avg reward: 6.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  27%|██▋       | 814/3000 [07:56<27:52,  1.31it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  30%|██▉       | 890/3000 [08:45<19:39,  1.79it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  30%|███       | 900/3000 [08:50<17:33,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900/3000 | Max reward: 22.00 | Avg reward: 6.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  33%|███▎      | 1000/3000 [09:49<24:56,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/3000 | Max reward: 22.00 | Avg reward: 6.25\n",
      "Model saved to ./models/reinforce_ep1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  37%|███▋      | 1100/3000 [10:52<21:01,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100/3000 | Max reward: 22.00 | Avg reward: 6.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  40%|███▉      | 1193/3000 [11:51<23:46,  1.27it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  40%|████      | 1200/3000 [11:55<16:57,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200/3000 | Max reward: 22.00 | Avg reward: 6.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  43%|████▎     | 1300/3000 [13:07<16:33,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300/3000 | Max reward: 22.00 | Avg reward: 6.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  45%|████▍     | 1349/3000 [13:44<13:11,  2.09it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  46%|████▋     | 1393/3000 [14:15<14:53,  1.80it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  47%|████▋     | 1400/3000 [14:19<14:34,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400/3000 | Max reward: 22.00 | Avg reward: 6.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  48%|████▊     | 1446/3000 [14:53<25:02,  1.03it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  50%|█████     | 1500/3000 [15:25<15:20,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500/3000 | Max reward: 22.00 | Avg reward: 6.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  53%|█████▎    | 1600/3000 [16:29<23:36,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600/3000 | Max reward: 22.00 | Avg reward: 6.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  54%|█████▍    | 1615/3000 [16:42<24:42,  1.07s/it]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  55%|█████▍    | 1639/3000 [16:56<11:42,  1.94it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  55%|█████▌    | 1664/3000 [17:10<18:19,  1.21it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  57%|█████▋    | 1700/3000 [17:35<10:59,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700/3000 | Max reward: 22.00 | Avg reward: 6.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  57%|█████▋    | 1707/3000 [17:40<12:27,  1.73it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  60%|██████    | 1800/3000 [18:37<11:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800/3000 | Max reward: 22.00 | Avg reward: 6.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  63%|██████▎   | 1900/3000 [19:46<15:34,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900/3000 | Max reward: 22.00 | Avg reward: 6.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  64%|██████▍   | 1932/3000 [20:10<18:13,  1.02s/it]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  67%|██████▋   | 2000/3000 [20:57<16:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000/3000 | Max reward: 22.00 | Avg reward: 6.89\n",
      "Model saved to ./models/reinforce_ep2000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  69%|██████▊   | 2059/3000 [21:37<09:25,  1.66it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  70%|███████   | 2100/3000 [22:04<11:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100/3000 | Max reward: 22.00 | Avg reward: 6.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  73%|███████▎  | 2200/3000 [23:09<05:36,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200/3000 | Max reward: 22.00 | Avg reward: 6.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  74%|███████▍  | 2227/3000 [23:27<10:39,  1.21it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  77%|███████▋  | 2300/3000 [24:08<06:22,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300/3000 | Max reward: 22.00 | Avg reward: 6.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  80%|████████  | 2400/3000 [25:14<07:35,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400/3000 | Max reward: 22.00 | Avg reward: 6.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  82%|████████▏ | 2466/3000 [25:57<07:03,  1.26it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  83%|████████▎ | 2500/3000 [26:21<05:58,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/3000 | Max reward: 22.00 | Avg reward: 6.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  84%|████████▎ | 2508/3000 [26:27<05:06,  1.61it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  87%|████████▋ | 2600/3000 [27:28<05:20,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600/3000 | Max reward: 22.00 | Avg reward: 7.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  90%|█████████ | 2700/3000 [28:30<02:48,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700/3000 | Max reward: 22.00 | Avg reward: 7.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  91%|█████████ | 2728/3000 [28:50<03:43,  1.22it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  92%|█████████▏| 2756/3000 [29:07<02:52,  1.42it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent:  93%|█████████▎| 2800/3000 [29:34<02:14,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800/3000 | Max reward: 22.00 | Avg reward: 7.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  97%|█████████▋| 2900/3000 [30:48<00:59,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900/3000 | Max reward: 22.00 | Avg reward: 7.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  99%|█████████▊| 2959/3000 [31:34<00:26,  1.55it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent: 100%|█████████▉| 2985/3000 [31:52<00:11,  1.35it/s]/Users/ryanchen/Desktop/UCSD/Fourth Year/Spring 2025/CSE 190/cse-190-final-project/algorithms/reinforce.py:65: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
      "Training REINFORCE Agent: 100%|██████████| 3000/3000 [31:59<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000/3000 | Max reward: 22.00 | Avg reward: 7.07\n",
      "Model saved to ./models/reinforce_ep3000.pth\n",
      "Training completed. Avg reward: 7.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent.train(\n",
    "    env=env,\n",
    "    num_episodes=config['num_episodes_train'],\n",
    "    print_freq=config['print_freq'],\n",
    "    save_freq=config['save_freq'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366be4a0",
   "metadata": {},
   "source": [
    "## Save Model Weights if Desired\n",
    "#### Highest reward runs during training are automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad0ccd",
   "metadata": {},
   "source": [
    "## Evaluate Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1f0abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True}\n",
      "action: {'type': 'ContinuousAction'}\n",
      "simulation_frequency: 15\n",
      "policy_frequency: 2\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 50\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -100.0\n",
      "high_speed_reward: 0.0\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: False\n",
      "offroad_terminal: True\n",
      "vehicle: {'acceleration': 3.0, 'steering': 0.4}\n",
      "collision_terminal: True\n"
     ]
    }
   ],
   "source": [
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de498d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf63d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
