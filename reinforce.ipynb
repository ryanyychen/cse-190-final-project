{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d913db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dda4c5",
   "metadata": {},
   "source": [
    "# REINFORCE (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration:\n",
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n",
      "Model configuration:\n",
      "{'hidden_size': 64, 'learning_rate': 0.001, 'gamma': 0.8, 'num_episodes_train': 3000, 'print_freq': 100, 'save_freq': 1000, 'num_episodes_eval': 100, 'top_k': 5}\n",
      "State size: 70, Action size: 2\n",
      "Using device: cuda\n",
      "Model loaded from ./models/reinforce_ep1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   1%|          | 1/100 [00:02<04:13,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed with reward 1.00 and 69 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   2%|▏         | 2/100 [00:03<02:37,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed with reward 1.00 and 46 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   3%|▎         | 3/100 [00:07<04:19,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 completed with reward 61.00 and 136 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   4%|▍         | 4/100 [00:08<03:18,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed with reward 1.00 and 51 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   5%|▌         | 5/100 [00:09<02:37,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed with reward 1.00 and 42 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   6%|▌         | 6/100 [00:10<02:18,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 completed with reward 1.00 and 51 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   7%|▋         | 7/100 [00:11<02:13,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 completed with reward 1.00 and 58 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   8%|▊         | 8/100 [00:13<02:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 completed with reward 1.00 and 48 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   9%|▉         | 9/100 [00:14<02:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 completed with reward 1.00 and 57 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  10%|█         | 10/100 [00:19<03:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 completed with reward 87.00 and 158 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  11%|█         | 11/100 [00:20<02:47,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  12%|█▏        | 12/100 [00:21<02:32,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 completed with reward 1.00 and 56 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  13%|█▎        | 13/100 [00:23<02:40,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 completed with reward 1.00 and 61 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  14%|█▍        | 14/100 [00:24<02:15,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 completed with reward 1.00 and 44 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  15%|█▌        | 15/100 [00:25<01:58,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 completed with reward 1.00 and 44 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  16%|█▌        | 16/100 [00:26<01:53,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 completed with reward 1.00 and 57 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  17%|█▋        | 17/100 [00:28<02:01,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 completed with reward 1.00 and 77 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  18%|█▊        | 18/100 [00:29<01:40,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 completed with reward 2.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  19%|█▉        | 19/100 [00:30<01:40,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 completed with reward 1.00 and 54 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  20%|██        | 20/100 [00:31<01:35,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 completed with reward 1.00 and 52 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  21%|██        | 21/100 [00:32<01:25,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 completed with reward 1.00 and 31 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  22%|██▏       | 22/100 [00:33<01:29,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 completed with reward 1.00 and 53 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  23%|██▎       | 23/100 [00:37<02:29,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 completed with reward 35.00 and 130 frames\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### One-cell script to run evaluation\n",
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "# MODEL_PATH = './models/reinforce_ep1000.pth'\n",
    "MODEL_PATH = './models/reinforce_best.pth'\n",
    "\n",
    "\n",
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Set rendering parameters\n",
    "# eval_env.config.update({\n",
    "#     'offscreen_rendering': False,\n",
    "#     'real_time_rendering': True,\n",
    "#     'render_agent': True,\n",
    "#     'show_trajectories': False\n",
    "# })\n",
    "\n",
    "\n",
    "# Display env configs\n",
    "print(\"Environment configuration:\")\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')\n",
    "\n",
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    print(\"Model configuration:\")\n",
    "    print(config)\n",
    "\n",
    "state_size = np.prod(eval_env.observation_space.shape)\n",
    "action_size = eval_env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.load_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42ad35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "MODEL_PATH = './models/reinforce.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5855faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode=None,\n",
    ")\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in env.config.keys():\n",
    "    print(f'{key}: {env.config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9723e3d",
   "metadata": {},
   "source": [
    "## Load Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b824c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 64\n",
      "learning_rate: 0.001\n",
      "gamma: 0.8\n",
      "num_episodes_train: 3000\n",
      "print_freq: 100\n",
      "save_freq: 1000\n",
      "num_episodes_eval: 100\n",
      "top_k: 5\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "for key in config.keys():\n",
    "    print(f'{key}: {config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94399219",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1f9e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 70, Action size: 2\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "state_size = np.prod(env.observation_space.shape)\n",
    "action_size = env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6572a8",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64d96ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   0%|          | 1/3000 [00:00<39:05,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 1.00 at episode 1\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   3%|▎         | 100/3000 [00:45<42:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 1.11 (consistency ratio: 0.09)\n",
      "Max reward: 12.00 at episode 100\n",
      "Model saved to ./models/reinforce_best.pth\n",
      "Episode 100/3000 | Max reward: 12.00 | Avg reward: 1.11 | Recent avg: 1.11 | Consistency ratio: 0.09 | Entropy coef: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▍         | 141/3000 [01:05<45:19,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 1.26 (consistency ratio: 0.08)\n",
      "Max reward: 16.00 at episode 141\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▌         | 150/3000 [01:10<36:27,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 1.28 (consistency ratio: 0.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▌         | 151/3000 [01:13<1:05:28,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 1.85 (consistency ratio: 0.03)\n",
      "Max reward: 58.00 at episode 151\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   5%|▌         | 162/3000 [01:20<37:14,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 1.93 (consistency ratio: 0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   6%|▌         | 174/3000 [01:29<1:08:32,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 2.62 (consistency ratio: 0.04)\n",
      "Max reward: 70.00 at episode 174\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   7%|▋         | 198/3000 [01:44<48:11,  1.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 2.63 (consistency ratio: 0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   7%|▋         | 200/3000 [01:46<45:30,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/3000 | Max reward: 70.00 | Avg reward: 1.81 | Recent avg: 2.52 | Consistency ratio: 0.04 | Entropy coef: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   7%|▋         | 202/3000 [01:51<1:33:45,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 3.44 (consistency ratio: 0.04)\n",
      "Max reward: 93.00 at episode 202\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   8%|▊         | 233/3000 [02:11<54:27,  1.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 3.79 (consistency ratio: 0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   8%|▊         | 236/3000 [02:14<57:47,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 3.96 (consistency ratio: 0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   8%|▊         | 238/3000 [02:16<57:15,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 4.05 (consistency ratio: 0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   8%|▊         | 245/3000 [02:23<40:16,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 7.13 (consistency ratio: 0.02)\n",
      "Max reward: 324.00 at episode 246\n",
      "Model saved to ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   8%|▊         | 249/3000 [02:49<2:50:54,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best recent average: 7.72 (consistency ratio: 0.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  10%|█         | 301/3000 [03:25<34:34,  1.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/3000 | Max reward: 324.00 | Avg reward: 3.38 | Recent avg: 6.52 | Consistency ratio: 0.02 | Entropy coef: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:  12%|█▏        | 345/3000 [03:55<30:15,  1.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance degradation detected! Loading previous best model...\n",
      "Model loaded from ./models/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 4]], which is output 0 of AsStridedBackward0, is at version 348; expected version 347 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_episodes_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprint_freq\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msave_freq\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\algorithms\\reinforce.py:223\u001b[39m, in \u001b[36mREINFORCEAgent.train\u001b[39m\u001b[34m(self, env, num_episodes, print_freq, save_freq)\u001b[39m\n\u001b[32m    220\u001b[39m     no_improvement_count += \u001b[32m1\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Update policy after each episode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# If no improvement for too long, increase exploration\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_improvement_count > \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\algorithms\\reinforce.py:127\u001b[39m, in \u001b[36mREINFORCEAgent.update_policy\u001b[39m\u001b[34m(self, rewards, log_probs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Update the policy\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Clip gradients to prevent exploding gradients\u001b[39;00m\n\u001b[32m    130\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), max_norm=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 4]], which is output 0 of AsStridedBackward0, is at version 348; expected version 347 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "agent.train(\n",
    "    env=env,\n",
    "    num_episodes=config['num_episodes_train'],\n",
    "    print_freq=config['print_freq'],\n",
    "    save_freq=config['save_freq'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366be4a0",
   "metadata": {},
   "source": [
    "## Save Model Weights if Desired\n",
    "#### Highest reward runs during training are automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad0ccd",
   "metadata": {},
   "source": [
    "## Evaluate Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f0abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf63d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./models/reinforce_best.pth\n"
     ]
    }
   ],
   "source": [
    "agent.load_model(\n",
    "    model_path='./models/reinforce_best.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de498d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   1%|          | 1/100 [00:00<01:23,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed with reward 18.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   2%|▏         | 2/100 [00:01<01:16,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed with reward 19.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   3%|▎         | 3/100 [00:03<01:49,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 completed with reward 1.00 and 59 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   4%|▍         | 4/100 [00:04<01:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed with reward 13.00 and 43 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   5%|▌         | 5/100 [00:05<01:35,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed with reward 30.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   6%|▌         | 6/100 [00:05<01:24,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   7%|▋         | 7/100 [00:06<01:24,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 completed with reward 14.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   8%|▊         | 8/100 [00:07<01:34,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 completed with reward 1.00 and 52 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   9%|▉         | 9/100 [00:08<01:21,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  10%|█         | 10/100 [00:09<01:16,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 completed with reward 1.00 and 34 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  11%|█         | 11/100 [00:10<01:18,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 completed with reward 1.00 and 41 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  12%|█▏        | 12/100 [00:10<01:11,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  13%|█▎        | 13/100 [00:11<01:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 completed with reward 10.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  14%|█▍        | 14/100 [00:12<01:07,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 completed with reward 8.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  15%|█▌        | 15/100 [00:12<01:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 completed with reward 13.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  16%|█▌        | 16/100 [00:14<01:16,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 completed with reward 1.00 and 62 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  17%|█▋        | 17/100 [00:14<01:09,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 completed with reward 9.00 and 33 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  18%|█▊        | 18/100 [00:17<01:51,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 completed with reward 19.00 and 88 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  19%|█▉        | 19/100 [00:18<01:31,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 completed with reward 24.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  20%|██        | 20/100 [00:18<01:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  21%|██        | 21/100 [00:19<01:11,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 completed with reward 8.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  22%|██▏       | 22/100 [00:20<01:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 completed with reward 12.00 and 33 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  23%|██▎       | 23/100 [00:20<01:01,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  24%|██▍       | 24/100 [00:21<01:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 completed with reward 1.00 and 39 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  25%|██▌       | 25/100 [00:22<01:05,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 completed with reward 1.00 and 42 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  26%|██▌       | 26/100 [00:23<01:03,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  27%|██▋       | 27/100 [00:24<01:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26 completed with reward 1.00 and 38 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  28%|██▊       | 28/100 [00:27<01:43,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 completed with reward 8.00 and 109 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  29%|██▉       | 29/100 [00:28<01:30,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28 completed with reward 1.00 and 37 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  30%|███       | 30/100 [00:28<01:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  31%|███       | 31/100 [00:29<01:06,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  32%|███▏      | 32/100 [00:30<00:59,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 completed with reward 3.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  33%|███▎      | 33/100 [00:30<00:54,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  34%|███▍      | 34/100 [00:31<00:52,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 completed with reward 1.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  35%|███▌      | 35/100 [00:32<00:49,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34 completed with reward 2.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  36%|███▌      | 36/100 [00:32<00:47,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  37%|███▋      | 37/100 [00:33<00:46,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36 completed with reward 10.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  38%|███▊      | 38/100 [00:34<00:44,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 completed with reward 6.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  39%|███▉      | 39/100 [00:36<01:15,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38 completed with reward 10.00 and 87 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  40%|████      | 40/100 [00:37<01:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39 completed with reward 13.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  41%|████      | 41/100 [00:38<01:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 completed with reward 1.00 and 45 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  42%|████▏     | 42/100 [00:39<00:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41 completed with reward 19.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  43%|████▎     | 43/100 [00:39<00:49,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42 completed with reward 23.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  44%|████▍     | 44/100 [00:41<01:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during episode 43: 'NoneType' object has no attribute 'get_image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  45%|████▌     | 45/100 [00:42<01:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44 completed with reward 1.00 and 45 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  46%|████▌     | 46/100 [00:43<01:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45 completed with reward 1.00 and 43 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  47%|████▋     | 47/100 [00:45<00:59,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46 completed with reward 1.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  47%|████▋     | 47/100 [00:46<00:52,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_episodes_eval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\algorithms\\reinforce.py:202\u001b[39m, in \u001b[36mREINFORCEAgent.evaluate\u001b[39m\u001b[34m(self, env, num_episodes, top_k, video_dir)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Scale action to range of environment's action space\u001b[39;00m\n\u001b[32m    200\u001b[39m action = action * [env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m], env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m next_state, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m total_reward += reward\n\u001b[32m    204\u001b[39m state = next_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\env.py:25\u001b[39m, in \u001b[36mGymnasiumRenderWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     obs, obs_info, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\env.py:105\u001b[39m, in \u001b[36mEnvWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     obs, obs_info, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.offroad_terminal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.vehicle.on_road:\n\u001b[32m    107\u001b[39m         done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\envs\\intersection_env.py:136\u001b[39m, in \u001b[36mIntersectionEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_vehicles()\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m._spawn_vehicle(spawn_probability=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mspawn_probability\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:272\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_type.act(action)\n\u001b[32m    271\u001b[39m \u001b[38;5;28mself\u001b[39m.road.act()\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimulation_frequency\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:31\u001b[39m, in \u001b[36mRegulatedRoad.step\u001b[39m\u001b[34m(self, dt)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps % \u001b[38;5;28mint\u001b[39m(\u001b[32m1\u001b[39m / dt / \u001b[38;5;28mself\u001b[39m.REGULATION_FREQUENCY) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menforce_road_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().step(dt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:50\u001b[39m, in \u001b[36mRegulatedRoad.enforce_road_rules\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vehicles) - \u001b[32m1\u001b[39m):\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vehicles)):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_conflict_possible\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m             yielding_vehicle = \u001b[38;5;28mself\u001b[39m.respect_priorities(\n\u001b[32m     52\u001b[39m                 \u001b[38;5;28mself\u001b[39m.vehicles[i], \u001b[38;5;28mself\u001b[39m.vehicles[j]\n\u001b[32m     53\u001b[39m             )\n\u001b[32m     54\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     55\u001b[39m                 yielding_vehicle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     56\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(yielding_vehicle, ControlledVehicle)\n\u001b[32m     57\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(yielding_vehicle, MDPVehicle)\n\u001b[32m     58\u001b[39m             ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:88\u001b[39m, in \u001b[36mRegulatedRoad.is_conflict_possible\u001b[39m\u001b[34m(v1, v2, horizon, step)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_conflict_possible\u001b[39m(\n\u001b[32m     82\u001b[39m     v1: ControlledVehicle,\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m     step: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.25\u001b[39m,\n\u001b[32m     86\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     87\u001b[39m     times = np.arange(step, horizon, step)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     positions_1, headings_1 = \u001b[43mv1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_trajectory_constant_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     positions_2, headings_2 = v2.predict_trajectory_constant_speed(times)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m position_1, heading_1, position_2, heading_2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m     92\u001b[39m         positions_1, headings_1, positions_2, headings_2\n\u001b[32m     93\u001b[39m     ):\n\u001b[32m     94\u001b[39m         \u001b[38;5;66;03m# Fast spherical pre-check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:196\u001b[39m, in \u001b[36mVehicle.predict_trajectory_constant_speed\u001b[39m\u001b[34m(self, times)\u001b[39m\n\u001b[32m    194\u001b[39m v.act(action)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m dt:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     positions.append(v.position.copy())\n\u001b[32m    198\u001b[39m     headings.append(v.heading)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:153\u001b[39m, in \u001b[36mVehicle.step\u001b[39m\u001b[34m(self, dt)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.heading += \u001b[38;5;28mself\u001b[39m.speed * np.sin(beta) / (\u001b[38;5;28mself\u001b[39m.LENGTH / \u001b[32m2\u001b[39m) * dt\n\u001b[32m    152\u001b[39m \u001b[38;5;28mself\u001b[39m.speed += \u001b[38;5;28mself\u001b[39m.action[\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m] * dt\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:172\u001b[39m, in \u001b[36mVehicle.on_state_update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.road:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         \u001b[38;5;28mself\u001b[39m.lane_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheading\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28mself\u001b[39m.lane = \u001b[38;5;28mself\u001b[39m.road.network.get_lane(\u001b[38;5;28mself\u001b[39m.lane_index)\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.road.record_history:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\road.py:69\u001b[39m, in \u001b[36mRoadNetwork.get_closest_lane_index\u001b[39m\u001b[34m(self, position, heading)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict.items():\n\u001b[32m     68\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m             distances.append(\u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     70\u001b[39m             indexes.append((_from, _to, _id))\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np.argmin(distances))]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\lane.py:141\u001b[39m, in \u001b[36mAbstractLane.distance_with_heading\u001b[39m\u001b[34m(self, position, heading, heading_weight)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distance(position)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m s, r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m angle = np.abs(\u001b[38;5;28mself\u001b[39m.local_angle(heading, s))\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) + \u001b[38;5;28mmax\u001b[39m(s - \u001b[38;5;28mself\u001b[39m.length, \u001b[32m0\u001b[39m) + \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m - s, \u001b[32m0\u001b[39m) + heading_weight * angle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\lane.py:212\u001b[39m, in \u001b[36mStraightLane.local_coordinates\u001b[39m\u001b[34m(self, position)\u001b[39m\n\u001b[32m    210\u001b[39m delta = position - \u001b[38;5;28mself\u001b[39m.start\n\u001b[32m    211\u001b[39m longitudinal = np.dot(delta, \u001b[38;5;28mself\u001b[39m.direction)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m lateral = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
