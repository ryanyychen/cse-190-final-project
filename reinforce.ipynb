{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d913db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dda4c5",
   "metadata": {},
   "source": [
    "# REINFORCE (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf68b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration:\n",
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o2\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n",
      "Model configuration:\n",
      "{'hidden_size': 64, 'learning_rate': 0.001, 'gamma': 0.8, 'num_episodes_train': 3000, 'print_freq': 100, 'save_freq': 1000, 'num_episodes_eval': 100, 'top_k': 5}\n",
      "State size: 70, Action size: 2\n",
      "Using device: cuda\n",
      "Model loaded from ./models_o2/reinforce_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   1%|          | 1/100 [00:01<02:41,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   2%|▏         | 2/100 [00:02<02:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed with reward 1.00 and 42 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   3%|▎         | 3/100 [00:03<01:36,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   4%|▍         | 4/100 [00:04<01:27,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed with reward 7.00 and 31 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   5%|▌         | 5/100 [00:04<01:17,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   6%|▌         | 6/100 [00:05<01:14,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   7%|▋         | 7/100 [00:06<01:21,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 completed with reward 1.00 and 49 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   8%|▊         | 8/100 [00:07<01:15,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 completed with reward 16.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   9%|▉         | 9/100 [00:07<01:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 completed with reward 24.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  10%|█         | 10/100 [00:08<01:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 completed with reward 1.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  11%|█         | 11/100 [00:10<01:41,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 completed with reward 1.00 and 62 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  12%|█▏        | 12/100 [00:11<01:28,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 completed with reward 13.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  13%|█▎        | 13/100 [00:12<01:23,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 completed with reward 15.00 and 35 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  14%|█▍        | 14/100 [00:12<01:16,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  15%|█▌        | 15/100 [00:15<01:48,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 completed with reward 1.00 and 74 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  16%|█▌        | 16/100 [00:16<01:39,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 completed with reward 1.00 and 38 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  17%|█▋        | 17/100 [00:16<01:25,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 completed with reward 7.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  18%|█▊        | 18/100 [00:17<01:17,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  19%|█▉        | 19/100 [00:18<01:14,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 completed with reward 6.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  20%|██        | 20/100 [00:19<01:26,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 completed with reward 6.00 and 59 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  21%|██        | 21/100 [00:20<01:17,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### One-cell script to run evaluation\n",
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "# MODEL_PATH = './models/reinforce_ep1000.pth'\n",
    "# MODEL_PATH = './models/reinforce_best.pth'\n",
    "MODEL_PATH = './models_o2/reinforce_best.pth'\n",
    "# MODEL_PATH = './models_o2/reinforce_ep1000.pth'\n",
    "\n",
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Set rendering parameters\n",
    "# eval_env.config.update({\n",
    "#     'offscreen_rendering': False,\n",
    "#     'real_time_rendering': True,\n",
    "#     'render_agent': True,\n",
    "#     'show_trajectories': False\n",
    "# })\n",
    "\n",
    "\n",
    "# Display env configs\n",
    "print(\"Environment configuration:\")\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')\n",
    "\n",
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    print(\"Model configuration:\")\n",
    "    print(config)\n",
    "\n",
    "state_size = np.prod(eval_env.observation_space.shape)\n",
    "action_size = eval_env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.load_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")\n",
    "\n",
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ad35ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "from env import create_env\n",
    "from algorithms.reinforce import REINFORCEAgent\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SEED = 42\n",
    "ENV_CONFIG = './configs/env.yaml'\n",
    "MODEL_CONFIG = './configs/reinforce.yaml'\n",
    "MODEL_PATH = './models_o2/reinforce.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5855faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o2\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode=None,\n",
    ")\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in env.config.keys():\n",
    "    print(f'{key}: {env.config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9723e3d",
   "metadata": {},
   "source": [
    "## Load Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b824c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 64\n",
      "learning_rate: 0.001\n",
      "gamma: 0.8\n",
      "num_episodes_train: 3000\n",
      "print_freq: 100\n",
      "save_freq: 1000\n",
      "num_episodes_eval: 100\n",
      "top_k: 5\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_CONFIG, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "for key in config.keys():\n",
    "    print(f'{key}: {config[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94399219",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f9e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 70, Action size: 2\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "state_size = np.prod(env.observation_space.shape)\n",
    "action_size = env.action_space.shape[0]\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "agent = REINFORCEAgent(\n",
    "    state_size=state_size,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    action_size=action_size,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    gamma=config['gamma'],\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6572a8",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d96ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   0%|          | 1/3000 [00:00<42:51,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 1.00 at episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training REINFORCE Agent:   0%|          | 3/3000 [00:02<47:42,  1.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_episodes_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprint_freq\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msave_freq\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\algorithms\\reinforce.py:163\u001b[39m, in \u001b[36mREINFORCEAgent.train\u001b[39m\u001b[34m(self, env, num_episodes, print_freq, save_freq)\u001b[39m\n\u001b[32m    160\u001b[39m action = action.detach().numpy()\n\u001b[32m    161\u001b[39m action = action * [env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m], env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m next_obs, obs_info, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m log_probs.append(log_prob)\n\u001b[32m    165\u001b[39m rewards.append(reward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\env.py:286\u001b[39m, in \u001b[36mEnvWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     obs, obs_info, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.offroad_terminal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.vehicle.on_road:\n\u001b[32m    288\u001b[39m         done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\envs\\intersection_env.py:136\u001b[39m, in \u001b[36mIntersectionEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_vehicles()\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m._spawn_vehicle(spawn_probability=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mspawn_probability\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:242\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    240\u001b[39m \u001b[38;5;28mself\u001b[39m._simulate(action)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n\u001b[32m    244\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m._is_terminated()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\observation.py:250\u001b[39m, in \u001b[36mKinematicObservation.observe\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m close_vehicles:\n\u001b[32m    249\u001b[39m     origin = \u001b[38;5;28mself\u001b[39m.observer_vehicle \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.absolute \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     vehicles_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserve_intentions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobserve_intentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_vehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvehicles_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     df = pd.concat([df, vehicles_df], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    258\u001b[39m df = df[\u001b[38;5;28mself\u001b[39m.features]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:2542\u001b[39m, in \u001b[36mDataFrame.from_records\u001b[39m\u001b[34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[39m\n\u001b[32m   2539\u001b[39m     columns = columns.drop(exclude)\n\u001b[32m   2541\u001b[39m manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m mgr = \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_mgr(mgr, axes=mgr.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2142\u001b[39m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[32m0\u001b[39m].shape, axes, e)\n\u001b[32m   2143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[32m-> \u001b[39m\u001b[32m2144\u001b[39m     \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2301\u001b[39m     new_values = new_values[argsort]\n\u001b[32m   2302\u001b[39m     new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m     bp = \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement=bp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2307\u001b[39m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.train(\n",
    "    env=env,\n",
    "    num_episodes=config['num_episodes_train'],\n",
    "    print_freq=config['print_freq'],\n",
    "    save_freq=config['save_freq'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366be4a0",
   "metadata": {},
   "source": [
    "## Save Model Weights if Desired\n",
    "#### Highest reward runs during training are automatically saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\n",
    "    model_path=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad0ccd",
   "metadata": {},
   "source": [
    "## Evaluate Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f0abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: {'type': 'Kinematics', 'vehicles_count': 10, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'sorted': True, 'normalize': True, 'include_road_info': True, 'include_vehicle_info': True, 'include_goal_info': True, 'history_length': 5}\n",
      "action: {'type': 'ContinuousAction', 'continuous': True, 'normalize': True, 'clip_actions': True}\n",
      "simulation_frequency: 10\n",
      "policy_frequency: 10\n",
      "other_vehicles_type: highway_env.vehicle.behavior.IDMVehicle\n",
      "screen_width: 600\n",
      "screen_height: 600\n",
      "centering_position: [0.5, 0.6]\n",
      "scaling: 7.15\n",
      "show_trajectories: False\n",
      "render_agent: True\n",
      "offscreen_rendering: False\n",
      "manual_control: False\n",
      "real_time_rendering: False\n",
      "duration: 100\n",
      "destination: o1\n",
      "controlled_vehicles: 1\n",
      "initial_vehicle_count: 10\n",
      "spawn_probability: 0.6\n",
      "collision_reward: -20.0\n",
      "high_speed_reward: 0.1\n",
      "arrived_reward: 50.0\n",
      "reward_speed_range: [0.0, 3.0]\n",
      "normalize_reward: True\n",
      "offroad_terminal: True\n",
      "vehicle: {'length': 4.5, 'width': 2.0, 'max_speed': 20.0, 'acceleration': 2.0, 'steering': 0.5, 'min_speed': 0.0, 'max_steering_angle': 0.5, 'wheel_base': 2.7, 'max_acceleration': 2.0, 'max_deceleration': 4.0, 'max_steering_speed': 0.5}\n",
      "collision_terminal: True\n",
      "off_road_reward: -5.0\n",
      "lane_keeping_reward: 0.5\n",
      "rewards: {'collision_reward': -20.0, 'high_speed_reward': 0.1, 'arrived_reward': 50.0, 'normalize_reward': True, 'off_road_penalty': -5.0, 'steering_penalty': -0.1, 'acceleration_penalty': -0.05, 'progress_reward': 0.5, 'distance_reward': 0.1, 'speed_reward': 0.2, 'heading_reward': 0.3}\n",
      "termination: {'max_steps': 500, 'min_steps': 30, 'max_off_road_time': 2.0, 'max_collision_time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "eval_env = create_env(\n",
    "    config_filepath=ENV_CONFIG,\n",
    "    render_mode='rgb_array',\n",
    ")\n",
    "eval_env.reset(seed=SEED)\n",
    "\n",
    "# Display env configs\n",
    "for key in eval_env.config.keys():\n",
    "    print(f'{key}: {eval_env.config[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf63d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./models/reinforce_best.pth\n"
     ]
    }
   ],
   "source": [
    "agent.load_model(\n",
    "    model_path='./models/reinforce_best.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de498d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   1%|          | 1/100 [00:00<01:23,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed with reward 18.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   2%|▏         | 2/100 [00:01<01:16,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed with reward 19.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   3%|▎         | 3/100 [00:03<01:49,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 completed with reward 1.00 and 59 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   4%|▍         | 4/100 [00:04<01:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed with reward 13.00 and 43 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   5%|▌         | 5/100 [00:05<01:35,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed with reward 30.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   6%|▌         | 6/100 [00:05<01:24,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   7%|▋         | 7/100 [00:06<01:24,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 completed with reward 14.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   8%|▊         | 8/100 [00:07<01:34,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 completed with reward 1.00 and 52 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:   9%|▉         | 9/100 [00:08<01:21,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  10%|█         | 10/100 [00:09<01:16,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 completed with reward 1.00 and 34 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  11%|█         | 11/100 [00:10<01:18,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 completed with reward 1.00 and 41 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  12%|█▏        | 12/100 [00:10<01:11,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  13%|█▎        | 13/100 [00:11<01:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 completed with reward 10.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  14%|█▍        | 14/100 [00:12<01:07,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 completed with reward 8.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  15%|█▌        | 15/100 [00:12<01:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 completed with reward 13.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  16%|█▌        | 16/100 [00:14<01:16,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 completed with reward 1.00 and 62 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  17%|█▋        | 17/100 [00:14<01:09,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 completed with reward 9.00 and 33 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  18%|█▊        | 18/100 [00:17<01:51,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 completed with reward 19.00 and 88 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  19%|█▉        | 19/100 [00:18<01:31,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 completed with reward 24.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  20%|██        | 20/100 [00:18<01:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  21%|██        | 21/100 [00:19<01:11,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 completed with reward 8.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  22%|██▏       | 22/100 [00:20<01:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 completed with reward 12.00 and 33 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  23%|██▎       | 23/100 [00:20<01:01,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 completed with reward 21.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  24%|██▍       | 24/100 [00:21<01:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 completed with reward 1.00 and 39 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  25%|██▌       | 25/100 [00:22<01:05,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 completed with reward 1.00 and 42 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  26%|██▌       | 26/100 [00:23<01:03,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 completed with reward 17.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  27%|██▋       | 27/100 [00:24<01:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26 completed with reward 1.00 and 38 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  28%|██▊       | 28/100 [00:27<01:43,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 completed with reward 8.00 and 109 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  29%|██▉       | 29/100 [00:28<01:30,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28 completed with reward 1.00 and 37 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  30%|███       | 30/100 [00:28<01:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  31%|███       | 31/100 [00:29<01:06,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  32%|███▏      | 32/100 [00:30<00:59,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 completed with reward 3.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  33%|███▎      | 33/100 [00:30<00:54,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32 completed with reward 14.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  34%|███▍      | 34/100 [00:31<00:52,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 completed with reward 1.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  35%|███▌      | 35/100 [00:32<00:49,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34 completed with reward 2.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  36%|███▌      | 36/100 [00:32<00:47,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35 completed with reward 11.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  37%|███▋      | 37/100 [00:33<00:46,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36 completed with reward 10.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  38%|███▊      | 38/100 [00:34<00:44,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 completed with reward 6.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  39%|███▉      | 39/100 [00:36<01:15,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38 completed with reward 10.00 and 87 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  40%|████      | 40/100 [00:37<01:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39 completed with reward 13.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  41%|████      | 41/100 [00:38<01:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 completed with reward 1.00 and 45 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  42%|████▏     | 42/100 [00:39<00:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41 completed with reward 19.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  43%|████▎     | 43/100 [00:39<00:49,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42 completed with reward 23.00 and 30 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  44%|████▍     | 44/100 [00:41<01:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during episode 43: 'NoneType' object has no attribute 'get_image'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  45%|████▌     | 45/100 [00:42<01:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44 completed with reward 1.00 and 45 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  46%|████▌     | 46/100 [00:43<01:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45 completed with reward 1.00 and 43 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  47%|████▋     | 47/100 [00:45<00:59,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46 completed with reward 1.00 and 40 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating REINFORCE Agent:  47%|████▋     | 47/100 [00:46<00:52,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_episodes_eval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\algorithms\\reinforce.py:202\u001b[39m, in \u001b[36mREINFORCEAgent.evaluate\u001b[39m\u001b[34m(self, env, num_episodes, top_k, video_dir)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Scale action to range of environment's action space\u001b[39;00m\n\u001b[32m    200\u001b[39m action = action * [env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m], env.config[\u001b[33m\"\u001b[39m\u001b[33mvehicle\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m next_state, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m total_reward += reward\n\u001b[32m    204\u001b[39m state = next_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\env.py:25\u001b[39m, in \u001b[36mGymnasiumRenderWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     obs, obs_info, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\env.py:105\u001b[39m, in \u001b[36mEnvWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     obs, obs_info, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.offroad_terminal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.vehicle.on_road:\n\u001b[32m    107\u001b[39m         done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\envs\\intersection_env.py:136\u001b[39m, in \u001b[36mIntersectionEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_vehicles()\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m._spawn_vehicle(spawn_probability=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mspawn_probability\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:272\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_type.act(action)\n\u001b[32m    271\u001b[39m \u001b[38;5;28mself\u001b[39m.road.act()\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimulation_frequency\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:31\u001b[39m, in \u001b[36mRegulatedRoad.step\u001b[39m\u001b[34m(self, dt)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps % \u001b[38;5;28mint\u001b[39m(\u001b[32m1\u001b[39m / dt / \u001b[38;5;28mself\u001b[39m.REGULATION_FREQUENCY) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menforce_road_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().step(dt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:50\u001b[39m, in \u001b[36mRegulatedRoad.enforce_road_rules\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vehicles) - \u001b[32m1\u001b[39m):\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vehicles)):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_conflict_possible\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m             yielding_vehicle = \u001b[38;5;28mself\u001b[39m.respect_priorities(\n\u001b[32m     52\u001b[39m                 \u001b[38;5;28mself\u001b[39m.vehicles[i], \u001b[38;5;28mself\u001b[39m.vehicles[j]\n\u001b[32m     53\u001b[39m             )\n\u001b[32m     54\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     55\u001b[39m                 yielding_vehicle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     56\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(yielding_vehicle, ControlledVehicle)\n\u001b[32m     57\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(yielding_vehicle, MDPVehicle)\n\u001b[32m     58\u001b[39m             ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\regulation.py:88\u001b[39m, in \u001b[36mRegulatedRoad.is_conflict_possible\u001b[39m\u001b[34m(v1, v2, horizon, step)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_conflict_possible\u001b[39m(\n\u001b[32m     82\u001b[39m     v1: ControlledVehicle,\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m     step: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.25\u001b[39m,\n\u001b[32m     86\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     87\u001b[39m     times = np.arange(step, horizon, step)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     positions_1, headings_1 = \u001b[43mv1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_trajectory_constant_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     positions_2, headings_2 = v2.predict_trajectory_constant_speed(times)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m position_1, heading_1, position_2, heading_2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m     92\u001b[39m         positions_1, headings_1, positions_2, headings_2\n\u001b[32m     93\u001b[39m     ):\n\u001b[32m     94\u001b[39m         \u001b[38;5;66;03m# Fast spherical pre-check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:196\u001b[39m, in \u001b[36mVehicle.predict_trajectory_constant_speed\u001b[39m\u001b[34m(self, times)\u001b[39m\n\u001b[32m    194\u001b[39m v.act(action)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m dt:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     positions.append(v.position.copy())\n\u001b[32m    198\u001b[39m     headings.append(v.heading)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:153\u001b[39m, in \u001b[36mVehicle.step\u001b[39m\u001b[34m(self, dt)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.heading += \u001b[38;5;28mself\u001b[39m.speed * np.sin(beta) / (\u001b[38;5;28mself\u001b[39m.LENGTH / \u001b[32m2\u001b[39m) * dt\n\u001b[32m    152\u001b[39m \u001b[38;5;28mself\u001b[39m.speed += \u001b[38;5;28mself\u001b[39m.action[\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m] * dt\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:172\u001b[39m, in \u001b[36mVehicle.on_state_update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.road:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         \u001b[38;5;28mself\u001b[39m.lane_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheading\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28mself\u001b[39m.lane = \u001b[38;5;28mself\u001b[39m.road.network.get_lane(\u001b[38;5;28mself\u001b[39m.lane_index)\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.road.record_history:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\road.py:69\u001b[39m, in \u001b[36mRoadNetwork.get_closest_lane_index\u001b[39m\u001b[34m(self, position, heading)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict.items():\n\u001b[32m     68\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m             distances.append(\u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     70\u001b[39m             indexes.append((_from, _to, _id))\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np.argmin(distances))]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\lane.py:141\u001b[39m, in \u001b[36mAbstractLane.distance_with_heading\u001b[39m\u001b[34m(self, position, heading, heading_weight)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distance(position)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m s, r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m angle = np.abs(\u001b[38;5;28mself\u001b[39m.local_angle(heading, s))\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) + \u001b[38;5;28mmax\u001b[39m(s - \u001b[38;5;28mself\u001b[39m.length, \u001b[32m0\u001b[39m) + \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m - s, \u001b[32m0\u001b[39m) + heading_weight * angle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CSE190\\cse-190-final-project\\venv\\Lib\\site-packages\\highway_env\\road\\lane.py:212\u001b[39m, in \u001b[36mStraightLane.local_coordinates\u001b[39m\u001b[34m(self, position)\u001b[39m\n\u001b[32m    210\u001b[39m delta = position - \u001b[38;5;28mself\u001b[39m.start\n\u001b[32m    211\u001b[39m longitudinal = np.dot(delta, \u001b[38;5;28mself\u001b[39m.direction)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m lateral = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent.evaluate(\n",
    "    env=eval_env,\n",
    "    num_episodes=config['num_episodes_eval'],\n",
    "    top_k=config['top_k'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
